{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-3e01991d",
        "language": "markdown"
      },
      "source": [
        "# üìß Syst√®me Intelligent de Filtrage des Emails (Spam vs Ham)",
        "",
        "Ce projet vise √† d√©velopper un syst√®me intelligent capable de filtrer les emails en d√©tectant s‚Äôils sont des spams ou non (ham), en utilisant des techniques de traitement automatique du langage (NLP) et de classification supervis√©e.",
        "",
        "",
        "## üì¶ 1. Importation des biblioth√®ques",
        "",
        "Dans cette section, nous importons toutes les biblioth√®ques n√©cessaires au projet : traitement des donn√©es, visualisations, NLP, Machine Learning et sauvegarde de mod√®les. Ces outils vont nous permettre de structurer le pipeline de classification d‚Äôemails de mani√®re rigoureuse et efficace.",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-456c9719",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 1. üì¶ Importation des biblioth√®ques",
        "# ===============================",
        "import pandas as pd",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "import nltk",
        "import re",
        "import string",
        "from nltk.corpus import stopwords",
        "from nltk.stem import PorterStemmer",
        "from nltk.tokenize import word_tokenize",
        "from wordcloud import WordCloud",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV",
        "from sklearn.feature_extraction.text import TfidfVectorizer",
        "from sklearn.metrics import classification_report, confusion_matrix",
        "from sklearn.naive_bayes import MultinomialNB",
        "from sklearn.tree import DecisionTreeClassifier",
        "from sklearn.svm import SVC",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-51af45fa",
        "language": "markdown"
      },
      "source": [
        "",
        "",
        "## üì• 2. Chargement et nettoyage des donn√©es",
        "",
        "Nous chargeons le jeu de donn√©es contenant les emails et leurs √©tiquettes (spam ou ham), puis proc√©dons √† un nettoyage de base :",
        "- V√©rification des colonnes",
        "- Suppression des doublons",
        "- Suppression des valeurs manquantes",
        "",
        "Cela permet de garantir la qualit√© des donn√©es avant toute analyse ou mod√©lisation.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-fb5fb391",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 2. üì• Chargement & Nettoyage des donn√©es",
        "# ===============================",
        "df = pd.read_csv(\"../data/DataSet_Emails.csv\")",
        "# delete duplicated lines",
        "",
        "print(\"Initial shape of the dataset:\", df.shape)",
        "print(\"nb de lighne dup\", df.duplicated().sum())",
        "df.drop_duplicates(inplace=True)",
        "# Drop rows with NaN ",
        "nb= df.shape[0]",
        "print(\"le nb du nan\", df.isnull().sum().sum())",
        "df.dropna(inplace=True)",
        "print(\"lignes supprimer :\", nb-df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-5367dc53",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üìä 3. Analyse exploratoire des donn√©es (EDA)",
        "",
        "L‚ÄôEDA permet de comprendre la structure du jeu de donn√©es :",
        "- Dimensions, types, aper√ßu des premi√®res lignes",
        "- R√©partition des classes (spam vs ham)",
        "- Analyse visuelle √† travers des nuages de mots pour identifier les termes fr√©quents dans chaque type d‚Äôemail",
        "",
        "Cette √©tape est cruciale pour orienter les choix de mod√©lisation.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-5774b18a",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 3. üìä Analyse exploratoire (EDA)",
        "# ===============================",
        "# dimensions et aper√ßu des donn√©es",
        "print(\"Dimensions:\", df.shape)",
        "",
        "df.head()",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-c879c2fd",
        "language": "python"
      },
      "source": [
        "nb= df['label_text'].value_counts()",
        "print(\"Distribution des emails:\\n\", nb)",
        "# distribution des classes",
        "sns.countplot(x='label', data=df)",
        "plt.title(\"Distribution des classes\")",
        "plt.show()",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-f3c3668d",
        "language": "python"
      },
      "source": [
        "",
        "# ===============================",
        "# generation des nuages de mots",
        "# ===============================",
        "def generate_wordcloud(data, label):",
        "    text = \" \".join(data[data['label_text'] == label]['text'])",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(text)",
        "    plt.imshow(wc, interpolation='bilinear')",
        "    plt.axis('off')",
        "    plt.title(f\"WordCloud - {label}\")",
        "    plt.show()",
        "generate_wordcloud(df, 'spam')",
        "generate_wordcloud(df, 'ham')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-c60a29cf",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üßΩ 4. Pr√©traitement des textes (NLP)",
        "",
        "Avant de vectoriser les emails, nous appliquons plusieurs transformations textuelles :",
        "- Conversion en minuscules",
        "- Tokenisation",
        "- Suppression des stopwords, de la ponctuation et des caract√®res sp√©ciaux",
        "- Stemming (r√©duction √† la racine des mots)",
        "",
        "Ces √©tapes permettent de normaliser le texte et d'am√©liorer la qualit√© de la repr√©sentation vectorielle.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-97b0c11d",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 4. üßΩ Pr√©traitement du texte (NLP)",
        "# ===============================",
        "nltk.download('punkt')",
        "nltk.download('stopwords')",
        "",
        "stemmer = PorterStemmer()",
        "stop_words = set(stopwords.words('english'))",
        "",
        "def preprocess(text):",
        "    text = text.lower()",
        "    text = re.sub(r'\\W', ' ', text)",
        "    tokens = word_tokenize(text)",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and word not in string.punctuation]",
        "    return ' '.join(tokens)",
        "",
        "df['clean_text'] = df['text'].apply(preprocess)",
        "df.dropna()",
        "df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-7c885c1e",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üî¢ 5. Extraction des caract√©ristiques",
        "",
        "Nous utilisons la m√©thode TF-IDF (Term Frequency-Inverse Document Frequency) pour convertir les textes en vecteurs num√©riques exploitables par les mod√®les de Machine Learning.",
        "",
        "Nous s√©parons ensuite les donn√©es en variables explicatives `X` et variable cible `y`, puis divisons le jeu de donn√©es en ensemble d'entra√Ænement et de test.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-50d4e47e",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 5. üî¢ Extraction des caract√©ristiques",
        "# ===============================",
        "vectorizer = TfidfVectorizer()",
        "X = vectorizer.fit_transform(df['clean_text'])",
        "y = df['label']",
        "",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-32d34016",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## ü§ñ 6. Entra√Ænement de plusieurs mod√®les",
        "",
        "Nous testons diff√©rents algorithmes de classification supervis√©e :",
        "- Naive Bayes",
        "- Arbre de D√©cision (Decision Tree)",
        "- SVM (Support Vector Machine)",
        "",
        "Le but est de comparer leurs performances afin de s√©lectionner le plus performant.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-06bad610",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 6. ü§ñ Entra√Ænement de plusieurs mod√®les",
        "# ===============================",
        "models = {",
        "    'NaiveBayes': MultinomialNB(),",
        "    'SVM': SVC(kernel='linear'),",
        "    'DecisionTree': DecisionTreeClassifier()",
        "}",
        "",
        "for name, model in models.items():",
        "    model.fit(X_train, y_train)",
        "    y_pred = model.predict(X_test)",
        "    print(f\"\\n{name} Results:\")",
        "    print(classification_report(y_test, y_pred))",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-ee36cf3d",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üìà 7. √âvaluation des mod√®les",
        "",
        "√Ä l‚Äôaide de m√©triques standards :",
        "- Matrice de confusion",
        "- Pr√©cision",
        "- Rappel",
        "- F1-score",
        "",
        "Nous √©valuons la qualit√© de chaque mod√®le sur le jeu de test, afin d‚Äôavoir une vision claire de leurs performances respectives.",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-52baa626",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üîÅ 8. Validation crois√©e (Cross-validation)",
        "",
        "Pour renforcer la robustesse de l‚Äô√©valuation, nous utilisons une validation crois√©e √† 5 plis. Cela permet de tester le mod√®le sur diff√©rentes sous-parties du dataset et de v√©rifier sa stabilit√©.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-2458d1d9",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 8. üîÅ Validation crois√©e",
        "# ===============================",
        "from sklearn.model_selection import cross_val_score",
        "for name, model in models.items():",
        "    scores = cross_val_score(model, X_train, y_train , cv=5 , scoring='f1')",
        "    print(f\"{name }: f1 score = {scores.mean():.3f} (+/- {scores.std():.3f})\")",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-9853126c",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üîß 9. Optimisation des hyperparam√®tres",
        "",
        "Nous utilisons `GridSearchCV` pour optimiser les param√®tres du mod√®le SVM. Cette m√©thode permet de tester plusieurs combinaisons de param√®tres pour identifier celle qui donne les meilleurs r√©sultats.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-d1d4be75",
        "language": "python"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV",
        "from sklearn.svm import SVC",
        "",
        "# Define your parameter distributions",
        "param_dist = {",
        "    'C': [0.1, 1, 10],                     # or use continuous: e.g. uniform(0.1, 10)",
        "    'kernel': ['linear', 'rbf'],",
        "    'gamma': ['scale', 0.01, 0.001]",
        "}",
        "",
        "# Setup RandomizedSearchCV",
        "random_search = RandomizedSearchCV(",
        "    estimator=SVC(),",
        "    param_distributions=param_dist,",
        "    n_iter=6,           # number of random combinations to try",
        "    cv=5,",
        "    scoring='accuracy',",
        "    random_state=42,",
        "    verbose=2,",
        "    n_jobs=-1",
        ")",
        "",
        "random_search.fit(X_train, y_train)",
        "",
        "print(\"Best SVM Parameters:\", random_search.best_params_)",
        "print(\"Best Cross‚ÄëVal Score:\", random_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-a5946e29",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üíæ 10. Sauvegarde du meilleur mod√®le",
        "",
        "Le meilleur mod√®le et le vectoriseur TF-IDF sont sauvegard√©s avec `joblib`, afin de pouvoir √™tre r√©utilis√©s dans une application (Streamlit par exemple) sans devoir les r√©entra√Æner.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "#VSC-06a329f5",
        "language": "python"
      },
      "source": [
        "# ===============================",
        "# 10. üíæ Sauvegarde du meilleur mod√®le",
        "# ===============================",
        "best_model = random_search.best_estimator_",
        "joblib.dump(best_model, '../models/model.pkl')",
        "joblib.dump(vectorizer, '../models/vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-b4041978",
        "language": "markdown"
      },
      "source": [
        "",
        "---",
        "",
        "## üåê 11. Interface utilisateur avec Streamlit",
        "",
        "Nous cr√©erons une interface simple avec Streamlit permettant √† l'utilisateur d‚Äôentrer un texte d‚Äôemail et d‚Äôobtenir en retour une pr√©diction :",
        "- Zone de saisie de texte",
        "- Bouton \"Analyser\"",
        "- Affichage clair : Spam ou Ham",
        "- (Optionnel) Affichage de la probabilit√©",
        "",
        "Cette interface permettra une utilisation interactive du mod√®le en production.",
        "",
        ""
      ]
    }
  ]
}